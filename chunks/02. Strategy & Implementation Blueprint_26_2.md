- **Member Understanding:** Part of trust is knowing how the AI is constrained. We educate members (through the Compass Guide and training) on what the AI will and won’t do. For example, we clarify that “The AI assistant is a helpful tool but it doesn’t know you personally or judge you; it’s bound by rules. If it ever refuses to answer or corrects you, it’s not being rude – it’s following safety protocols we designed. And if you think it’s wrong, you have the right to challenge that with a human.” This way, members don’t develop blind trust in the AI nor unwarranted suspicion – they see it as another fallible part of the system with checks in place.  
- **External AI Inputs:** Another firewall aspect is isolating our AI from external control. We don’t allow third-party AI to directly inject into our systems without scrutiny. If, say, a government or another org offers data or an AI model to integrate, we evaluate it carefully, possibly run it sandboxed, and ensure it can’t override our safeguards. The AI firewall extends to not just content but the very code and models we run – everything gets vetted (this ties into gear vetting philosophy, but for software models).