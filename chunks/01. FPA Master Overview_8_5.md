- **AI Systems & Reliability:** The AI that we use in Phase2 and 3 – from the little recommendation engines to the big crisis simulation AIs – are developed or integrated by Tech & App Dev. They either build custom AI models or leverage open-source ones, training them on FPA’s collected data (like past incident reports, environmental data, etc.). A key mandate is ensuring **AI reliability and bias control**. This means they don’t treat AI as magic: they validate outputs, put in constraints to prevent the AI from suggesting unethical or out-of-scope actions, and they create **explainability** features (so AI doesn’t just say “do X” – it provides a rationale or confidence level). They program a sort of “ethical governor” into our AIs, aligning with Alliance values and the oversight guidelines drafted by the Governance content family. Moreover, they design UIs that make it easy for human operators to give feedback on AI suggestions (basically upvote/downvote or comment), which in turn is fed back into model refinement. They closely track AI performance in drills vs. human decisions to gauge when to trust it more or pull back. By Phase3, this team has likely fine-tuned AI to be a true asset – but they never stop monitoring it, because even a good AI can go bad if conditions change or if someone tries to tamper with it.