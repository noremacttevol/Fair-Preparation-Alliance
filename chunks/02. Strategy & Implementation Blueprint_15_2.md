- **Fail-safes:** In the tech stack, the AI is never a single point of failure. If the AI system is down or malfunctioning, the app clearly indicates it (“AI assistant unavailable”) and falls back to static info (the member can manually search the Compass Guide or contact a human). If the AI gives an answer, it often provides a citation or reference (like pointing to the section of the Guide it drew from) to maintain transparency. We’d rather the AI under-perform (and ask the user to seek human help) than overreach and give possibly wrong guidance. This conservative approach is built into its programming (one could say caution is a parameter in the model’s configuration).  
- **Member Control:** Members have control over how and when to use the AI. They can mute the AI’s proactive suggestions if they find it distracting. They can view the **AI moderation flags** on their content (so if the AI thinks something they wrote might be against policy, they see the flag and can appeal or discuss with a human moderator). This is important – moderation isn’t happening in a black box; the AI is a tool to help us enforce standards, but final judgments (like removing a member or issuing a warning) are made by humans via the Tribunal or appointed moderators. All AI actions in moderation (e.g., hiding a message pending review) are logged and reviewable to ensure the AI isn’t acting out of scope.