**What’s already implemented (current wins):** The AI Oversight Committee was established 3 months ago. They have since conducted two formal audits of the FPAi “Test-My-Plan” outputs. Wins: these audits found and fixed **three instances of subtle bias/inaccuracy** (for example, early versions undervalued community solutions and overemphasized individual stockpiling – likely due to training data skew. The committee caught this, and we retrained the AI on FPA’s community-oriented guidelines ([5 AI Auditing Frameworks to Encourage Accountability](https://auditboard.com/blog/ai-auditing-frameworks#:~:text=AI%20auditing%20ensures%20that%20organizations,design%2C%20development%2C%20deployment%2C%20and%20monitoring))). We’ve set up an **AI feedback form** within the app where any member after getting AI advice can rate it or flag concerns. This feedback goes directly to the Oversight team. Already, 100+ pieces of feedback were reviewed, leading to improvements in how the AI communicates (making it more encouraging, less authoritative in tone when uncertainty exists). Another success: the AI now provides **citations/links** with its advice into our Knowledge Base, which was a direct Oversight requirement for transparency. Members report feeling more confident in the AI’s suggestions because they can “see the sources” or rationale. The oversight group also created an **Ethical Use Policy** for FPAi, which is a plain-language document telling members what the AI will and won’t do (for