- Continuously improve the AI with feedback: encourage users to mark AI suggestions as helpful or not, and feed that back into the system’s learning process. Keep the AI’s knowledge of FPA values up-to-date by inputting our Trust Doctrine and any new policies so it can check alignment.
**Contingency Plan:** If the AI system fails or gives bad info (say it’s down during a disaster or makes a flawed recommendation), revert to normal human-led processes – we must ensure no one has become solely reliant on it. Always have a human double-check critical suggestions (“AI says send all water to X – does that really make sense?”). If AI flags a human communication as suspect erroneously (false positive), moderators should quickly clear it and perhaps adjust the sensitivity to maintain trust in the process. In case of any suspicion that the AI has been compromised (cyberattack altering outputs), we’d disconnect it until verified, and possibly run a backup on a secure offline machine with known good data. We also commit that if AI ever conflicts with the Trust Doctrine or our human sense of compassion, we side with human judgment – that principle is clearly stated in our guidelines. As a final backup, all AI suggestions are just that – suggestions. If the power’s out or the data is lost, FPA operated just fine before without it and can do so again; Phase 3’s resilience doesn’t hinge on AI, it’s just augmented by it.