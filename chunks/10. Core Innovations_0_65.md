they can “see the sources” or rationale. The oversight group also created an **Ethical Use Policy** for FPAi, which is a plain-language document telling members what the AI will and won’t do (for instance, it won’t give tactical offense advice or anything violating our ethics, and it won’t pretend to be human). By publishing this, we’ve set the right expectations. Importantly, we’ve avoided any major incident; no AI advice catastrophe has occurred, in part because we had humans in the loop from the start. The committee includes some skeptics which is good – they challenge improvements rigorously. For example, before expanding FPAi to all members, Oversight did a “red team” exercise trying to prompt the AI into giving bad advice; the findings from that led to additional safeguards in place.