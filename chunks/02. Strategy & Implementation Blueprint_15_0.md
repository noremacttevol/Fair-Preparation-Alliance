Our AI systems merit special emphasis in the stack because of their powerful role and the need for reliability:  
- **AI Components:** We have multiple AI components – the **Advisor AI** (helps members with plans and questions), the **Moderator AI** (monitors content and flags issues), and possibly an **Analyst AI** (looking at aggregated data, spotting patterns like emerging risks or training needs across the org). These might be separate modules or different modes of one system, but logically they have distinct functions.  
- **Integration with Stack:** The Advisor AI operates both locally and in the cloud as mentioned. The Moderator AI likely runs partly on the device (for quick feedback – e.g., if someone tries to post something disallowed, the app can catch it instantly offline through a rule set or a lightweight model) and partly on the server (for more nuanced analysis of aggregated communication when synced). The Analyst AI would run centrally, producing reports that leadership can review (for example, “80% of squads did not include X in their fire drill plan; this might indicate a training gap” – such insights can come from automated analysis of logs).  
- **Oversight and Tuning:** We have a dedicated **AI Reliability & Oversight Model**, which means a team or process that constantly evaluates the AI’s performance. This includes:  
- Testing the AI on known scenarios (did it give the correct advice?),  
- Monitoring its suggestions for errors or biases,