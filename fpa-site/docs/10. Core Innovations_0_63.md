**Why it was created (problem it solves):** As we integrated AI tools, we recognized the **risks**: AI could give incorrect or harmful suggestions, could be biased by bad data, or might not align with our ethos of fairness and safety. We needed to **preemptively build trust in AI** by making its use transparent and accountable – hence this oversight framework. It solves the problem of **AI governance** in our context: ensuring that members feel confident using FPAi and that its recommendations can be trusted because they’re being checked. It also mitigates legal/liability concerns; by having oversight and audit trails, we show due diligence in how AI is used for advice. Ultimately, it prevents blind faith in technology – reinforcing that AI is a tool, and final decisions rest with informed humans. _With great AI comes great accountability, and without comprehensive audit procedures, organizations risk bias, misuse, or worse ([5 AI Auditing Frameworks to Encourage Accountability](https://auditboard.com/blog/ai-auditing-frameworks#:~:text=like%20Siri%20and%20Alexa%20to,misuse%2C%20and%20worse%2C%20regulatory%20roadblocks)). Our protocols enforce accountability to minimize such risks._