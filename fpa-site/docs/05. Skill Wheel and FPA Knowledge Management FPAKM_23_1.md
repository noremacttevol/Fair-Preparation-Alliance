- **AI Reliability Plan:** FPA’s AI Reliability Plan (a set of guidelines for how we implement AI responsibly) has been a key reference for every AI-related feature in the Skill Wheel project. That plan likely emphasizes things like **explainability, human oversight, fail-safes, and data privacy**. We have baked those principles in throughout our design:  
- _Explainability:_ As noted, any AI-generated survivability score comes with an explanation of strengths and weaknesses, so members are never left mystified as to why the AI “thinks” something.  
- _Human Oversight:_ In Phases 2 and 3, AI recommendations are advisory and subject to instructor/mentor review. We are not letting the AI make high-stakes decisions (like promoting or excluding someone) without human confirmation. Early on especially, every AI action will be monitored by staff.  
- _Fail-Safes:_ If the AI system encounters an error or outputs something clearly off-base, the system will detect that (we can build in sanity checks – for instance if an output contradicts known data) and default to a safe state. A “safe state” might mean sticking with the last known good recommendation or deferring to a human. We will not allow an AI glitch to mislead our members or undermine trust. For example, if FPAi for some reason gave someone a bizarre 0% survivability warning (say due to a bug), we’d have checks to catch that and hold it for review rather than display it.